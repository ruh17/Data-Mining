---
title: "Home Work 3"
author: "Sai Rakesh Ghanta"
date: "February 12, 2017"
output: word_document

---
## Loading the required packages

```{r}
# hw3sample.R - sample code for hw3
# @author: Yu-Ru Lin
# @date: 2017-02-12
# Modified by Sai Rakesh Ghanta

suppressWarnings(library(MASS)) 
suppressWarnings(library(plyr)) # for recoding data
suppressWarnings(library(ROCR))  # for plotting roc
suppressWarnings(library(e1071))  # for NB and SVM
suppressWarnings(library(rpart))  # for decision tree
suppressWarnings(library(ada))  # for adaboost
suppressWarnings(library(car))  # for recode
suppressWarnings(library(class))   # kNN
suppressWarnings(library(RGtk2))  
suppressWarnings(library(rattle))   
suppressWarnings(library(rpart.plot))  
suppressWarnings(library(RColorBrewer))   
suppressWarnings(library(knitr)) # for Table
suppressWarnings(library(ggplot2)) # for bar charts
suppressWarnings(library(pROC)) # ROC
```

# 2) Report at least two variants for techniques with parameters and incorporate them into your table. For examples, for kNN, you may include kNN-1, kNN-3, kNN-5. For decision tree, you may include the default tree, and a tree after pruning. For SVM, you may include different kernels and gamma/cost parameters.

# Classification techniques with variants.

```{r}
do.classification <- function(train.set, test.set, cl.name, verbose=F) {
  
  switch(cl.name, 
         knn1 = { # here we test k=1; 
           prob = knn(train.set[,-10], test.set[,-10], cl=train.set[,10], k = 1, prob=T)
           prob = attr(prob,"prob")
           #print(cbind(prob,as.character(test.set$TARGET_Adjusted)))
           prob
         },
         knn3 = { # here we test k=3; 
           prob = knn(train.set[,-10], test.set[,-10], cl=train.set[,10], k = 3, prob=T)
           prob = attr(prob,"prob")
           #print(cbind(prob,as.character(test.set$TARGET_Adjusted)))
           prob
         },
         knn5 = { # here we test k=5; 
           prob = knn(train.set[,-10], test.set[,-10], cl=train.set[,10], k = 5, prob=T)
           prob = attr(prob,"prob")
           #print(cbind(prob,as.character(test.set$TARGET_Adjusted)))
           prob
         },
         lr = { # logistic regression (lr)
           model = glm(TARGET_Adjusted~., family=binomial, data=train.set)
           if (verbose) {
             print(summary(model))             
           }
           prob = predict(model, newdata=test.set, type="response") 
           #print(cbind(prob,as.character(test.set$TARGET_Adjusted)))
           prob
         },
         nb = { # naive Bayes
           model = naiveBayes(TARGET_Adjusted~., data=train.set)
           prob = predict(model, newdata=test.set, type="raw") 
           #print(cbind(prob,as.character(test.set$TARGET_Adjusted)))
           prob = prob[,2]/rowSums(prob) # renormalize the prob.
           prob
         },
         dtree = {
           model = rpart(TARGET_Adjusted~., data=train.set)
           if (verbose) {
             print(summary(model)) # detailed summary of splits
             printcp(model) # print the cross-validation results
             plotcp(model) # visualize the cross-validation results
             
           } 
           ## plot the tree
           
           plot(model, uniform=TRUE, main="Classification Tree") # default tree
           text(model, use.n=TRUE, all=TRUE, cex=.8)
           prob = predict(model, newdata=test.set)
           
           #print(cbind(prob,as.character(test.set$TARGET_Adjusted)))
           prob = prob[,2]/rowSums(prob) # renormalize the prob.
           prob
         },
         dtreeprune = { # Pruned Tree
           model = rpart(TARGET_Adjusted~., data=train.set)
           if (verbose) {
             print(summary(model)) # detailed summary of splits
             printcp(model) # print the cross-validation results
             plotcp(model) # visualize the cross-validation results
            
           }           
           prob = predict(model, newdata=test.set)
           
           ## prune the tree 
           
           pfit<- prune(model, cp=model$cptable[which.min(model$cptable[,"xerror"]),"CP"])
           prob = predict(pfit, newdata=test.set)
           
           ## plot the pruned tree 
           
           plot(pfit, uniform=TRUE,main="Pruned Classification Tree")
           text(pfit, use.n=TRUE, all=TRUE, cex=.8)          
           fancyRpartPlot(pfit)   
           
           #print(cbind(prob,as.character(test.set$TARGET_Adjusted)))
           prob = prob[,2]/rowSums(prob) # renormalize the prob.
           prob
         },
         svm = {
           model = svm(TARGET_Adjusted~., data=train.set, probability=T)
           if (0) { 
             # fine-tune the model with Radial kernel and parameters
             ## evaluate range of gamma parameter between 0.000001 and 0.1
             ## and cost parameter from 0.1 until 10
             
             tuned <- tune.svm(TARGET_Adjusted~., data = train.set, 
                               kernel="radial", 
                               gamma = 10^(-6:-1), cost = 10^(-1:1))
             
             #print(summary(tuned))
             
             gamma = tuned[['best.parameters']]$gamma
             cost = tuned[['best.parameters']]$cost
             
             model = svm(TARGET_Adjusted~., data = train.set, probability=T, 
                         kernel="radial", gamma=gamma, cost=cost)    
             
           }
           prob = predict(model, newdata=test.set, probability=T)
           prob = attr(prob,"probabilities")
           #print(cbind(prob,as.character(test.set$TARGET_Adjusted)))
           #print(dim(prob))
           prob = prob[,which(colnames(prob)==1)]/rowSums(prob)
           prob
         },
         svm1 = {
           model = svm(TARGET_Adjusted~., data=train.set, probability=T)
           if (0) { 
             # fine-tune the model with polynomial kernel and parameters
             ## evaluate range of gamma parameter between 0.000002 and 0.2
             ## and cost parameter from 0.2 until 20
             
             tuned <- tune.svm(TARGET_Adjusted~., data = train.set, 
                               kernel="polynomial", 
                               gamma = 20^(-6:-1), cost = 20^(-1:1))
             
             #print(summary(tuned))
             
             gamma = tuned[['best.parameters']]$gamma
             cost = tuned[['best.parameters']]$cost
             
             model = svm(TARGET_Adjusted~., data = train.set, probability=T, 
                         kernel="radial", gamma=gamma, cost=cost)                        
           }
           prob = predict(model, newdata=test.set, probability=T)
           prob = attr(prob,"probabilities")
           #print(cbind(prob,as.character(test.set$TARGET_Adjusted)))
           #print(dim(prob))
           prob = prob[,which(colnames(prob)==1)]/rowSums(prob)
           prob
         },
         ada = {
           model = ada(TARGET_Adjusted~., data = train.set)
           prob = predict(model, newdata=test.set, type='probs')
           #print(cbind(prob,as.character(test.set$y)))
           prob = prob[,2]/rowSums(prob)
           prob
         }
  ) 
}
pre.test <- function(audit, cl.name, r=0.6, prob.cutoff=0.5) {
  ## Let's use 60% random sample as training and remaining as testing
  ## by default use 0.5 as cut-off
  n.obs <- nrow(audit) # no. of observations in dataset
  n.train = floor(n.obs*r)
  train.idx = sample(1:n.obs,n.train)
  train.idx
  train.set = audit[train.idx,]
  test.set = audit[-train.idx,]
  
  cat('pre-test',cl.name,':',
      '#training:',nrow(train.set),
      '#testing',nrow(test.set),'\n')
  
  prob = do.classification(train.set, test.set, cl.name)
  # prob is an array of probabilities for cases being positive
  
  ## get confusion matrix
  predicted = as.numeric(prob > prob.cutoff)
  actual = test.set$TARGET_Adjusted
  confusion.matrix = table(actual,factor(predicted,levels=c(0,1)))
  error = (confusion.matrix[1,2]+confusion.matrix[2,1]) / nrow(test.set)  
  cat('error rate:',error,'\n')
  
  
  ## plot ROC
  result = data.frame(prob,actual)
  pred = prediction(result$prob,result$actual)
  perf = performance(pred, "tpr","fpr")
  #plot(perf)    
}

k.fold.cv <- function(audit, cl.name, k.fold=10, prob.cutoff=0.5) {
  ## default: 10-fold CV, cut-off 0.5 
  n.obs <- nrow(audit) # no. of observations 
  s = sample(n.obs)
  errors = dim(k.fold)
  probs = NULL
  actuals = NULL
  for (k in 1:k.fold) {
    test.idx = which(s %% k.fold == (k-1) ) # use modular operator
    train.set = audit[-test.idx,]
    test.set = audit[test.idx,]
    cat(k.fold,'-fold CV run',k,cl.name,':',
        '#training:',nrow(train.set),
        '#testing',nrow(test.set),'\n')
    prob = do.classification(train.set, test.set, cl.name)
    
    predicted = as.numeric(prob > prob.cutoff)
    actual = test.set$TARGET_Adjusted
    confusion.matrix = table(actual,factor(predicted,levels=c(0,1)))
    confusion.matrix
    error = (confusion.matrix[1,2]+confusion.matrix[2,1]) / nrow(test.set)  
    errors[k] = error
    cat('\t\terror=',error,'\n')
    probs = c(probs,prob)
    actuals = c(actuals,actual)
    ## you may compute other measures and store them in arrays
  }
  avg.error = mean(errors)
  cat(k.fold,'-fold CV results:','avg error=',avg.error,'\n')
  
  ## plot ROC
  result = data.frame(probs,actuals)
  pred = prediction(result$probs,result$actuals)
  perf = performance(pred, "tpr","fpr")
  plot(perf)   
  
  ## get other measures by using 'performance'
  get.measure <- function(pred, measure.name='auc') {
    perf = performance(pred,measure.name)
    m <- unlist(slot(perf, "y.values"))
    #     print(slot(perf, "x.values"))
    #     print(slot(perf, "y.values"))
    m
  }
  err = mean(get.measure(pred, 'err')) 
  accuracy = mean(get.measure(pred, 'acc')) 
  precision = mean(get.measure(pred, 'prec'),na.rm=T) 
  recall = mean(get.measure(pred, 'rec'),na.rm=T) 
  fscore = mean(get.measure(pred, 'f'),na.rm=T) 
  cat('error=',err, 'accuracy=', accuracy, 'precision=',precision,'recall=',recall,'f-score',fscore,'\n')
  auc = get.measure(pred, 'auc')
  cat('auc=',auc,'\n')
  m1 <- data.frame(accuracy,precision,recall,fscore,auc)
  #return(m1) #get result 
  m2 <- c(m1,result)
  return(m2)
}

my.classifier <- function(audit, cl.name, do.cv=F) {
  n.obs <- nrow(audit) # no. of observations in dataset
  n.cols <- ncol(audit) # no. of predictors
  cat('my dataset:',
      n.obs,'observations',
      n.cols-1,'predictors','\n')
  print(audit[1:3,])
  cat('label (TARGET_Adjusted) distribution:')
  print(table(audit$TARGET_Adjusted))
  
  pre.test(audit, cl.name)
  if (do.cv) {
    stat <- k.fold.cv(audit, cl.name, k.fold = 10)
    return(stat)
  }
}
```

# 1) Use a 10-fold cross-validation to evaluate different classification techniques. Report your 10-fold CV classification results in a performance table. In the table, report the values of different performance measures for each classification technique.

```{r}
set.seed(1) # set the seed so you can get exactly the same results whenever you run the code

# loading the data set

audit <- read.csv('C:/Users/Sai Rakesh Ghanta/Desktop/audit.csv', header = TRUE, sep = ',')
head(audit)
audit$TARGET_Adjusted = as.factor(audit$TARGET_Adjusted)
summary(audit)

# dealing with missing data

any(is.na(audit))
audit <- na.omit(audit)
any(is.na(audit))
audit = audit[-which(audit$Employment == "Volunteer"),]
audit$Employment = droplevels(audit$Employment)  # drop unused levels
audit = audit[-which(audit$Occupation == "Military"),]
audit$Occupation = droplevels(audit$Occupation)  # drop unused levels
audit <- audit[-c(1,11)]

# Coerce (Recode) the categorical variables into discrete numbers

audit$Employment = recode(audit$Employment, "'Consultant'=1; 'Private'=2; 'PSFederal'=3; 'PSLocal'=4; 'PSState'=5; 'SelfEmp'=6")
audit$Education = recode(audit$Education, "'Associate'=1; 'Bachelor'=2; 'College'=3; 'Doctorate'=4; 'HSgrad'=5; 'Master'=6; 'Preschool'=7; 'Professional'=8; 'Vocational'=9; 'Yr10'=10; 'Yr11'=11; 'Yr12'=12; 'Yr1t4'=13; 'Yr5t6'=14; 'Yr7t8'=15; 'Yr9'=16")
audit$Marital = recode(audit$Marital, "'Absent'=1; 'Divorced'=2; 'Married'=3; 'Married-spouse-absent'=4; 'Unmarried'=5; 'Widowed'=6")
audit$Gender = recode(audit$Gender, "'Male'=1; else=0")
audit$Occupation = recode(audit$Occupation, "'Cleaner'=1; 'Clerical'=2; 'Executive'=3; 'Farming'=4; 'Home'=5; 'Machinist'=6; 'Professional'=7; 'Protective'=8; 'Repair'=9; 'Sales'=10; 'Service'=11; 'Support'=12; 'Transport'=13")

summary(audit)

# 10-fold CV classification results

result_lr <- my.classifier(audit, cl.name='lr',do.cv=T) # logistic regression
result_knn1 <- my.classifier(audit, cl.name='knn1',do.cv=T) # kNN-1
result_knn3 <- my.classifier(audit, cl.name='knn3',do.cv=T) # kNN-3
result_knn5 <- my.classifier(audit, cl.name='knn5',do.cv=T) # kNN-5
result_nb <- my.classifier(audit, cl.name='nb',do.cv=T)  #Naive Bayesian  
result_dtree <- my.classifier(audit, cl.name='dtree',do.cv=T) # Default Tree
result_dtreeprune <- my.classifier(audit, cl.name='dtreeprune',do.cv=T) # Pruned Tree
result_svm <- my.classifier(audit, cl.name='svm',do.cv=T) # SVM - Radial Kernel
result_svm1 <- my.classifier(audit, cl.name='svm1',do.cv=T) # SVM - Polynomial Kernel
result_ada <- my.classifier(audit, cl.name='ada',do.cv=T) # ADA

# values of different performance measures for each classification technique

stat <- data.frame(accuracy=double(),precision=double(),recall=double(),fscore=double(),auc=double(),stringsAsFactors=FALSE)

stat[1,1] = result_lr$accuracy
stat[1,2] = result_lr$precision
stat[1,3] = result_lr$recall
stat[1,4] = result_lr$fscore
stat[1,5] = result_lr$auc


stat[2,1] = result_knn1$accuracy
stat[2,2] = result_knn1$precision
stat[2,3] = result_knn1$recall
stat[2,4] = result_knn1$fscore
stat[2,5] = result_knn1$auc

stat[3,1] = result_knn3$accuracy
stat[3,2] = result_knn3$precision
stat[3,3] = result_knn3$recall
stat[3,4] = result_knn3$fscore
stat[3,5] = result_knn3$auc

stat[4,1] = result_knn5$accuracy
stat[4,2] = result_knn5$precision
stat[4,3] = result_knn5$recall
stat[4,4] = result_knn5$fscore
stat[4,5] = result_knn5$auc

stat[5,1] = result_nb$accuracy
stat[5,2] = result_nb$precision
stat[5,3] = result_nb$recall
stat[5,4] = result_nb$fscore
stat[5,5] = result_nb$auc

stat[6,1] = result_dtree$accuracy
stat[6,2] = result_dtree$precision
stat[6,3] = result_dtree$recall
stat[6,4] = result_dtree$fscore
stat[6,5] = result_dtree$auc

stat[7,1] = result_dtreeprune$accuracy
stat[7,2] = result_dtreeprune$precision
stat[7,3] = result_dtreeprune$recall
stat[7,4] = result_dtreeprune$fscore
stat[7,5] = result_dtreeprune$auc

stat[8,1] = result_svm$accuracy
stat[8,2] = result_svm$precision
stat[8,3] = result_svm$recall
stat[8,4] = result_svm$fscore
stat[8,5] = result_svm$auc

stat[9,1] = result_svm1$accuracy
stat[9,2] = result_svm1$precision
stat[9,3] = result_svm1$recall
stat[9,4] = result_svm1$fscore
stat[9,5] = result_svm1$auc

stat[10,1] = result_ada$accuracy
stat[10,2] = result_ada$precision
stat[10,3] = result_ada$recall
stat[10,4] = result_ada$fscore
stat[10,5] = result_ada$auc

# Performance Table

rownames(stat) <- c('lr','kNN-1','kNN-3','kNN-5','NB','Decision Tree','Pruned Tree','SVM','SVM1','ADA')
stat1 = as.data.frame(t(stat))
stat1 
kable(stat1, caption = 'Table: performance measures for each classification technique')

```

# Generate two bar charts, one for F-score and one for AUC, that allow for visually comparing different classification techniques.

```{r}
# F-Score Bar Chart
stat<-as.data.frame(stat)
Classification <- rownames(stat)
F_Score<-stat$fscore
AUC<-stat$auc
ggplot(data=stat, aes(x=Classification, y=F_Score, fill=Classification)) + geom_bar(stat="identity")

# AUC Bar Chart

ggplot(data=stat, aes(x=Classification, y=AUC, fill=Classification)) + geom_bar(stat="identity")

```

From the above bar plots & Performance Table we can see that F-Score is high for ADA model where as  AUC is gh for SVM with radial kernel.

# 3) Generate an ROC plot that plot the ROC curve of each model into the same figure and include a legend to indicate the name of each curve. For techniques with variants, plot the best curve that has the highest AUC.

```{r}
color.set <- c('red', 'yellow', 'green', 'blue', 'cyan', 'magenta')
cl.name.set <- c('lr','knn','nb','dtree','svm','ada')

Roc1=roc(result_lr$actuals, result_lr$probs, plot=TRUE, print.thres=TRUE, print.auc=TRUE)
Roc2=roc(result_knn1$actuals, result_knn1$probs, plot=TRUE, print.thres=TRUE, print.auc=TRUE)
Roc3=roc(result_nb$actuals, result_nb$probs, plot=TRUE, print.thres=TRUE, print.auc=TRUE)
Roc4=roc(result_dtree$actual, result_dtree$probs, plot=TRUE, print.thres=TRUE, print.auc=TRUE)
Roc5=roc(result_svm$actual, result_svm$probs, plot=TRUE, print.thres=TRUE, print.auc=TRUE)
Roc6=roc(result_ada$actual, result_ada$probs, plot=TRUE, print.thres=TRUE, print.auc=TRUE)
plot(Roc1, col="red") 
plot.roc(Roc2, add=TRUE, col="yellow") 
plot.roc(Roc3, add=TRUE, col="green") 
plot.roc(Roc4, add=TRUE, col="blue") 
plot.roc(Roc5, add=TRUE, col="cyan") 
plot.roc(Roc6, add=TRUE, col="magenta") 

legend("bottomright", legend=cl.name.set, col=color.set, lwd=6)
```

Comparing kNN variants, kNN-1 has better AUC value. 
Comparing dtree variants, default dtree has better AUC value. 
Comparing SVM variants, SVM with radial hernel has better AUC value. 

# 4) Summarize the model performance based on the table and the ROC plot in one or two paragraphs.

A Receiver Operating Characteristic Curve (ROC) is a standard technique for summarizing classifier performance over a range of trade-offs between true positive (TP) and false positive (FP) error rates. It is a graphical plot that illustrates the performance of a binary classifier system as its discrimination threshold is varied.The Area Under the Curve (AUC), also referred to as index of accuracy (A), is an accepted traditional performance metric for a ROC curve. The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test. The higher the area under the curve the better prediction power the model has.

From both ROC Plots and Performance Table of the classification techniques used for our data set 'Audit.csv' we can understand that SVM has better predictive power. SVM has the highest AUC value which means it predicts TARGET_Adjusted better in our data set. Also, recall which is the fraction of relevant instances that are retrieved is high in case of SVM. LR & ADA models also have AUC scores approximately equal to SVM making them better predictive models but SVM leads them in a minute difference.



